{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Classifying Songs in Spotify Playlists</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<center>Capstone Project for IBM Data Science Professional Certificate Specialization</center>__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<center>by Tauno Tanilas - 2019</center>__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Capstone Project for [IBM Data Science Professional Certificate](https://www.coursera.org/specializations/ibm-data-science-professional-certificate?recoOrder=4&utm_medium=email&utm_source=recommendations&utm_campaign=recommendationsEmail~recs_email~2019-01-07) course. The subject of final project in this course is left to the author's choice. I decided to apply my acquired knowledge into the field of Data Classification and implement it on the data available in Spotify platform.\n",
    "\n",
    "__Business problem__\n",
    "- Understand the user's needs and preferences in evaluating musical tracks.\n",
    "- Knowing these characteristics, provide better services.  \n",
    "\n",
    "__Target audience__\n",
    "- Audio streaming providers.\n",
    "- Any person willing to get insights about using classification modeling in solving machine learning problems.\n",
    "\n",
    "__Main goals__\n",
    "- Determine characteristics that define the user's musical taste.\n",
    "- Using these characteristics compare the music user likes or dislikes.\n",
    "- Create a predictive model on whether user likes or dislikes a song.\n",
    "- Determine the main musical genres that are in userâ€™s preferences.  \n",
    "- Using Foursquare API, construct a map of accommodation options in one of the selected concert places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is constructed from the Spotify playlists that I have created for myself. There are 80 playlists in total - 45 liked and 25 disliked for training and 10 for evaluation. Liked playlists are labeled as 'GOOD' and disliked as 'BAD'. For later evaluation purposes a separate file is for ten different musical genres that contain tracks that were not used in training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the info of playlists tracks and the audio features of them, I used the Spotipy API. The data used for modeling is described by the following ten audio features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Acousticness:__ A measure from 0.0 to 1.0 of whether the track is acoustic.  \n",
    "- __Energy:__ A measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy.  \n",
    "- __Danceability:__ Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.  \n",
    "- __Instrumentalness:__ Predicts whether a track contains no vocals. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content.  \n",
    "- __Liveness:__ Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.  \n",
    "- __Loudness:__ The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track. Values typical range between -60 and 0 db.  \n",
    "- __Speechiness:__ Detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talkshow, audio book, poetry), the closer to 1.0 the attribute value.  \n",
    "- __Tempo:__ The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.  \n",
    "- __Valence:__ A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).  \n",
    "- __Duration:__ The duration of the track in milliseconds.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
